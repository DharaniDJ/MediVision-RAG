# MediVision-RAG

### Project Explanation

This project is a **multimodal Retrieval-Augmented Generation (RAG) application** tailored for veterinary medical use cases, enabling users to query information across various data modalities—such as text, tables, and images—from medical PDFs. Built on **FastAPI**, the application integrates multiple services, including **GPT-4**, **LangChain**, and **FAISS**, to provide rich, context-specific answers. Here’s an in-depth breakdown of the project’s objectives, its implementation, and functionality:

---

#### **Objective**
The primary objective of this project is to allow users to obtain comprehensive answers to medical queries by combining text and images extracted from documents. This is particularly useful in veterinary medicine, where users (e.g., veterinarians, medical researchers) might need a combination of text-based explanations and images to understand medical conditions, such as canine periodontal diseases. The application enables efficient querying by leveraging **multimodal data processing** to enrich the answer quality, providing both textual descriptions and relevant visual context from PDF files.

---

#### **Implementation**
The project is implemented using a range of advanced technologies, each carefully chosen to optimize the application’s capabilities for multimodal content handling. Here’s a breakdown of each component and its purpose:

- **FastAPI**:
  - **Purpose**: FastAPI is a high-performance Python web framework ideal for building REST APIs. It is used here to manage API requests and responses.
  - **Reason for Use**: FastAPI provides asynchronous request handling and excellent support for JSON-based APIs, making it fast and suitable for applications that require high-speed processing. It also integrates well with modern Python libraries, supporting dependency injection and background tasks, which are beneficial for managing model inference and data retrieval operations in this app.

- **GPT-4 and GPT-3.5-turbo (OpenAI models)**:
  - **Purpose**: These language models are used to generate natural language responses based on the extracted content and user queries. **GPT-4** is primarily responsible for generating detailed and accurate descriptions of images, while **GPT-3.5-turbo** is used for text and table summarization.
  - **Reason for Use**: GPT-4 and GPT-3.5-turbo are robust, large language models that excel at processing and generating natural language responses based on complex inputs. GPT-4, in particular, supports multimodal capabilities, allowing it to analyze images and generate visual descriptions—an essential feature for this project where images from medical PDFs are used alongside text.

- **LangChain**:
  - **Purpose**: LangChain provides the orchestration framework that manages interactions between different models and modules in the app. It enables the creation of chains of prompts and processing pipelines, which are crucial for handling the text, table, and image summaries generated by the OpenAI models.
  - **Reason for Use**: LangChain simplifies the implementation of complex workflows by managing chains of language model queries and responses. It supports custom prompt creation and chaining, allowing efficient interactions with both the GPT-3.5-turbo and GPT-4 models. This is especially useful in structuring workflows for the app’s multimodal capabilities, as it allows the models to work together seamlessly for each type of data input.

- **Unstructured Library**:
  - **Purpose**: The Unstructured library is used to parse and extract content from PDF documents, specifically for handling text, tables, and images. It allows for different chunking strategies, such as breaking content by title, which enhances the ability to process large documents.
  - **Reason for Use**: Unstructured is a highly versatile library that supports complex document layouts and enables granular data extraction. Its ability to handle various elements within a document makes it ideal for parsing medical PDFs, which often contain detailed information across multiple formats (e.g., textual descriptions, tabular data, and images).

- **Tesseract OCR and Poppler Utils**:
  - **Purpose**: Tesseract OCR is utilized for optical character recognition on images extracted from the PDFs, while Poppler Utils aids in handling PDF content.
  - **Reason for Use**: These utilities are essential for extracting and interpreting text embedded within images or scanned PDFs. This is particularly important when working with medical documents that contain high-quality image scans, where text information may not always be in a readable PDF text layer.

- **FAISS (Facebook AI Similarity Search)**:
  - **Purpose**: FAISS is an open-source library for efficient similarity search and clustering of dense vectors. In this app, it serves as the vector store, indexing both text and image data for rapid retrieval based on semantic similarity.
  - **Reason for Use**: FAISS supports large-scale similarity search tasks and is optimized for high-performance querying of large datasets. Its capability to index and retrieve semantically similar content allows the application to quickly find and return relevant answers based on user queries. This is critical for improving the speed and accuracy of information retrieval in a RAG system, especially when dealing with extensive medical content.

---

#### **Functionality**
The application processes user queries by extracting relevant content from the PDF documents, summarizing the information, and then presenting it in a multimodal format. Here’s a step-by-step overview of the app’s functionality:

1. **Data Extraction and Processing**:
   - The `partition_pdf` function from the Unstructured library reads the PDF file, extracting text, tables, and images based on specified chunking parameters. The app uses a **by-title** chunking strategy to segment the content logically, which is particularly useful for handling sections in medical PDFs.
   - Extracted **text elements** and **table elements** are summarized using GPT-3.5-turbo via LangChain’s `LLMChain`. The summaries provide concise, relevant content, reducing the amount of text while retaining essential information for quick access.
   - Images are encoded in Base64 and passed to GPT-4 for visual summarization. The summaries generated by GPT-4 describe the contents of each image in medical terms, providing context for the visuals included in the query results.

2. **Vector Store Creation and Retrieval**:
   - Once the summaries are generated, the app uses the FAISS library to create a **vector database**. The `OpenAIEmbeddings` model indexes the summarized content, which facilitates fast similarity searches based on user queries.
   - Text, tables, and images are stored with metadata tags, allowing the app to retrieve specific content types efficiently. This capability enables the application to respond to multimodal queries by returning both textual and visual data, providing a rich, context-aware response.

3. **Query Handling and Response Generation**:
   - The app receives user queries through a POST request on the FastAPI endpoint. The `answer()` function then searches the FAISS vector store for the most relevant content based on semantic similarity to the query.
   - For each relevant document found, the application builds a response context that includes text, tables, and Base64-encoded images. These are passed into a **QA chain** that uses GPT-4 to generate a detailed, human-readable answer, enriched with visual aids when applicable.
   - Example: For a query like “What is gingivitis?”, the app retrieves a description of gingivitis from the relevant text and provides an image that visually represents the condition, offering the user a comprehensive, multimodal answer.

4. **User Interface and Interaction**:
   - Users interact with the app through a simple interface, where they can input queries and view detailed responses. The application’s multimodal response capability makes it ideal for educational and research purposes, particularly in fields like veterinary science, where images are often as crucial as textual descriptions for understanding medical conditions.

This application not only enhances the accessibility of medical information but also exemplifies how **Generative AI** can be applied to deliver intuitive and informative user experiences through multimodal data presentation.